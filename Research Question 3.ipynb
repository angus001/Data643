{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Research Question 3\n",
    "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the TOP new york hospitals, New York Presbyterian Hospital, has a 300 millions fund to invest in predictive analytics. The hospital also has an in-house data science team to build predictive models for the daily operations of the hospital. They quickly found there are biases of the algorithm or rather the systems does not align with our ethical values. Specifically, the hospital mentioned the model was recommending non-English speaker patients to be discharged early to reduce the length of stay in the hospital. A patient's length of stay is a quality measure often used to rate a hospital by government and other non-profit agency. This instance is very similar to the instance described in NYU article- \"When Recommendation Systems Go Bad\", where the model based discrimination happened using features such as gender, ethnicity and social class. The examples include lower paying jobs being recommended to women, higher price tag to MacBook owner and loan default algorithm descrimination due to race. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most recommendation systems’ current state of the art technique still uses similarities among users and products as building blocks for producing recommendations. These building blocks operate in a narrowly defined range. Since the systems assume people who share similar interest will remain sharing the same interest over time. It does not take into account of the evolving nature of people’s taste. A person who enjoys certain cuisine or certain theme park might certainly becomes disdain from such cuisine or theme park due to a food poisoning accident or violent equipment failure at the park. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the afore-mentioned hospital algorithm, it did cause unethical customer segmentation due to language barrier. The NYU (1) does mention a feature segregation methods and building separate models to avoid this kind of unethical targeting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference:\n",
    "\n",
    "1)https://cds.nyu.edu/recommendation-systems-go-bad-%E2%80%A8/ <br>\n",
    "2) https://arxiv.org/pdf/1610.02413.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
