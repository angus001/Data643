{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 643 Research Discussion 4\n",
    "\n",
    "\n",
    "<ins>Prompt</ins>: <br>\n",
    "As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered; do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Response</ins>: <br>\n",
    "“The most important thing in navigating to your destination is knowing where you are in the journey” – An old maxim. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that all three articles for algorithmic bias in recommender system agreed is that users and general population would be taken into extreme content based on their initial interest. An example would be a political conservative who favor Donald Trump would be exposed to white supremist content quite quickly. This kind of extreme content spans all facets of people interest; from ultramarathon to dietary styles. <br>\n",
    "\n",
    "<br>Based on the content we have covered in class, recommender systems in general are not biased. They are narrowly focused and use shortcut to drive people attention to a set particular objective quickly. This let to users becoming more passive passengers rather than an active driver in their interest discovery quest. As recommender systems are for people’s attention and engagement, it is prudent to take into account of human psychology. Humans are more prone to pay attention to stimuli that are more out of extraordinary than the norm. A quick browsing on YouTube platform trending videos will show all the top videos are quite extraordinary or at least have the “click-bait” or exaggerated picture of something not remotely close to the actual content.  <br>\n",
    "\n",
    "Somewhere in the development of the recommendation system, the algorithm has figured out we are drawn to this kind of extreme content. Naturally, the quickest way to constantly grab our attention is to recommend more and more extreme content within that category.  <br>\n",
    "\n",
    "Another reason for users to be more attracted to biased content is the information overload. Before Facebook, YouTube and Twitter have become the “go to” sources for information, it would hard to imagine how researching jogging could lead to watching videos about ultramarathon. As the digital contents are more readily available at our fingerprint and we become a more passive passengers, the contents have created an endless labyrinth designed to keep us in. Not knowing where we stand in the labyrinth, we are easily sidetracked to a deeper and darker part of it with one out of ordinary video at a time.\n",
    "The research paper by Krishnan et. al. (2014) suggests differentiating the recommended content if they are prone to influence bias. I agree with this approach as this will help users counter extreme contents by first letting them know where they are in their journey of searching so that they can get to their destination quickly. In addition, the content should be identified as to where in the spectrum of our social norm it falls into. A white supremist ideology would certainly fall into a small and far splice end of the philosophical spectrum; I hope, at least has not been normalized in this age of radicalized ideas.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>Reference:</ins> <br>\n",
    "- https://www.wired.com/story/creating-ethical-recommendation-engines/\n",
    "\n",
    "- https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html \n",
    "\n",
    "- http://goldberg.berkeley.edu/pubs/sanjay-recsys-v10.pdf \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
